---
title: "Sad Music"
author: "Marielle Baelemans"
date: "17 March 2019"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: lumen

---

```{r, cache = FALSE}


# In order to use these packages, we need to install flexdashboard, plotly, and Cairo.
library(tidyverse)
library(tidymodels)
library(plotly)
library(dplyr)
library(spotifyr)
library(compmus)
source('spotify.R')
```

### Sad Music?! (61)

Earlier research showed that minor, slowness and low pitch in music gives us a sad feeling.

![Results AOC](ResultsAO.jpg)

***

**Introduction**

For this project I want to use the findings out of an earlier Musicology project. In my research on what makes music 'sad' I found that the minor aspect is the thing what makes music sad. Besides that aspect, also slowness and low pitch are important for giving a listener a sad feeling. My big goal for this portfolio project is to make a widget that measures how sad someone's playlist is by comparing these factors in a 3:2:1 ratio. This is based on the results, as you can see in the graph.  With these outcomes I want to make a day by day scheme for how 'sad' your day was according to the songs you listened that day. A calendar that shows your mood based on the music you listen ;). For now this is a too big goal, for this project my research question is 

*'What is the mood of my own music, compared to most listenend pophits?*

So my first step is to compare the major/minor- ratio and the mean tempo of my own playlist 'Most listened 2018' to 'Top 50 Nederland' playlist. I have not found a way yet to measure the pitch of the songs in the playlists, so for now I use the 'loudness' as extra indicator.

**First Results**

So far it seems that my own playlist is less sad than the Top 50 Nederland. Minor is 31% against 40% in Top 50 Nederland. My mean tempo is 120 BPM (sd= 31.7) against 116 BPM (sd= 24,0) For loudness the outcome for my most listened 2018 has a mean of -8,06 ; sd= 3,55. For Top50NL it has a mean of -6,64 with sd= 2,84.

**Formula**

I can make a formule to calculate the sadness (because of the different in numbers this formulate is not really accurate, but it is a sketch for futher steps).

Sadness= 3mode -2(tempo/1000) -loudness/10.

Sadness(Top50NL)= 3x0,40 - 2x0,116 + 0,664 = 1.632

Sadness(My2018)= 3x0,31 - 2x0,120 + 0,806 = 1.976


**Goals**

My goal is to create a formule that which takes these aspects in a 3:2:1 ratio and is based on a 1 to 100 scale.

Imported to keep in mind for the next weeks is that aspects as tempo and loudness are not well measured for songs with long silent intros, like You- The 1975. Those kind of songs are better left behind. While googling for these statistics I found out that Spotify has a correctness chance, this is something to use in the next weeks.

I would also prefer to use Last.fm for my statistics, because I spend a big amount of my music listening on YouTube.



### 'You listen to such depressing music!' (72)
My own playlist (6,000 songs) comparement with a playlist of hits in all popgenres (10,000). 

![This is a JPEG image due of the large amount of data](ComparementGraph.png)

*** 

**Comparement and variables**

First of all, I decided to change the playlist I'm comparing. I have a playlist where I put in all the music I listen to. This playlist consist of almost 6,000 songs. I compare this playlist with a playlist that consists of 10.000, based on the most famous songs per genre. Good to notice is that this playlist is almost twice as big as my own playlist (10K versus 6K) It is hard to decide what kind of playlist is best fitting for my project. Because I'm comparing my own music to 'normal' music, I should have a playlist that consist of songs that are most listened, and known by the greatest amount of people.  

Dr. Burgoyne told us in the last lecture that Energy and Valence are mostly used in music cognition for measuring emotion is music. This made me change my way of doing it in the research, so for now on I'll use energy, valence, mode, loudness and tempo as variables.The example visualisation dr. Burgoyne made for our lecture was luckily for me fitting for my portfolio!

Based on what Dr. Burgoyne told in the lecture, for this research I will use: mode, tempo, loudness, energy and valence. Luckily for me, dr. Burgoyne already made a really good visualisation using these factors, so the only thing I had to do was chancing the visualisation to my own playlists. Besides I changed the way minor/minor was visualised in colors and added tempo to the alpha factor. 
 
**Conclusion**

While the 10K playlist makes a clear line from the left bottum corner to the right top corner, my playlist really clusters at the left. There is a cluster at the left bottum corner, which would mean that my music is partly 'sad'.  There's also a cluster at the middle of the buttom of this graph, what would be an 'anger' cluster. 

### 'Go down, soft sound' - Same song, different style, pitch? (83)
The differences between the three versions of 'The 1975' by the band The 1975. 

```{r}

  The1975ABIIOR <- 
    get_tidy_audio_analysis('7dvM0LbJ4pu1tDJnCH1Ahg') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)

  The1975ILWYSYSBYSUOI <- 
    get_tidy_audio_analysis('4LyEonRJ0clC020Yz3Qtk6') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
  
  The1975 <- 
    get_tidy_audio_analysis('2RWikgBoqBM5nu9GXPYNhq') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
  
```

```{r}
#To make the graphic
  The1975ABIIOR %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
      aes(
        x = start + duration / 2, 
        width = duration, 
        y = pitch_class, 
        fill = value
        )) + 
    geom_tile() +
    labs(x = 'Time (s)', y = 'The 1975 ABIIOR', fill = 'Magnitude') +
    theme_minimal()

  The1975ILWYSYSBYSUOI %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
      aes(
        x = start + duration / 2, 
        width = duration, 
        y = pitch_class, 
        fill = value
        )) + 
    geom_tile() +
    labs(x = 'Time (s)', y = 'The 1975 ILWYSYSBYSUOI', fill = 'Magnitude') +
    theme_minimal() 
   
  
 The1975 %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
      aes(
        x = start + duration / 2, 
        width = duration, 
        y = pitch_class, 
        fill = value
        )) + 
    geom_tile() +
    labs(x = 'Time (s)', y ='The 1975 The 1975', fill = 'Magnitude') +
    theme_minimal()
```


*** 

**Case Study: The 1975**

One thing is clear, my all time favorite artist is the British band The 1975. To go a bit deeper into the music I listen to, I take my favourite band as a case study for the next analyse technique. I can give a whole TedTalk why this is the best band ever, but I will give you one reason in this project.
Every album, they start off with the same song, 'The 1975'. Same lyrics, but different in style. As a prelude, the band introduces the style of the album with this song. But how much do these versions actually differ? Time to use the track analysis. 

**Conclusion**
As you can see there's a little difference in the pitches that are used in the three songs. Clear to see that the most recent version has repeated sequences. The second one has longer sequences, what is more used in an electronic style. And  the fist one, has the most build up sequence. 

### 'You're intertwining your soul with somebody else'- Comparement between versions (83)
Self-similarity of the normal and alt version of 'Somebody Else' by the band 'The 1975'. 

```{r}
SomebodyElseNormal <- 
    get_tidy_audio_analysis('4m0q0xQ2BNl9SCAGKyfiGZ') %>%
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
SomebodyElseAlt <- 
    get_tidy_audio_analysis('4SbbujQwsJZozLjbix7QZ2') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)

```

```{r}
compmus_long_distance(
    SomebodyElseNormal %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
    SomebodyElseAlt %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
    feature = pitches,
    method = 'euclidean') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    scale_fill_continuous(type = 'viridis', guide = "none") +
    labs(x = 'SomebodyElseNormal', y = 'SomebodyElseAlt') +
    theme_minimal()
```

*** 
**Alt versus normal version - Long distance comparement**
My most listened song of The 1975 is 'Somebody Else'. This song has two versions, a normal and an 'Alt' version. In my opinion, the  versions do not differ so much, by ear. Let's use this analysis to see whether I am right or not!

**Conclusion**
Somebody Else seems only to differ in the bridge part of the song, but in the rest of the song, the song is mostly the same. 

### The 'Colorness' of PopMusic - Timbre and Chroma (94)

***

**Style differences**

Another key charastic of The 1975 is their use of different styles between songs. My favourite song, 'Robbers' is quite a pop song. Their most famous song, 'Chocolate' is one of the most 'wannabe boyband popsong of the '10's' you could think of. However 'Please Be Naked' is a completely instrumental, in a piano/electronic way. 

**Conclusion**

As you can see out of the plots, the most colorfull one in all 12 sections is the instrumental 'Please Be Naked'. 'Chocolate' is almost completely blue and 'Robbers' has a little yellow, meaning that the more 'poppy' a song is, the less color in the timbre  sections has. 

### The Segmentation of PopMusic (105)
```{r}

circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))


Robbers<- 
    get_tidy_audio_analysis('73jVPicY2G9YHmzgjk69ae') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))%>% 
    compmus_match_pitch_template(key_templates, 'aitchison', 'manhattan')%>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'Robbers', fill = 'Distance')

ggplotly(Robbers)

PleaseBeNaked<- 
    get_tidy_audio_analysis('7jS5cQ5GuGkPPdIoUUct0P') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))%>% 
    compmus_match_pitch_template(key_templates, 'aitchison', 'manhattan')%>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'PleaseBeNaked', fill = 'Distance')

ggplotly(PleaseBeNaked)

Chocolate<- 
    get_tidy_audio_analysis('0KivRxhyT9xMen2cs3rJD3')%>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))%>% 
    compmus_match_pitch_template(key_templates, 'aitchison', 'manhattan')%>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'Chocolate', fill = 'Distance')

ggplotly(Chocolate)
```



***

**Style differences**

Using the same three songs of previous plot, you can also see that the popsongs have a clear bridge convention. This means that the song is verse-chorus-verse-BRIDGE-chorus. The 'Please Be Naked' song, however, has a clear segmentations in it, which means that there are way more sections within the song.There is even no verse, chorus or bridge. There is a repeated segment at the end, and that is it.

### Tonal Analysis
```{r}
Robbers %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')

Chocolate %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')

PleaseBeNaked %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '')
``` 
*** 

In the comparement of tonal analysis you can see that they are mostly the same. All blue graphs. 
### Low level features (116)

```{r}
Mijn<-
    get_playlist_audio_features(
        '11122548577','76MRLl4f6AvyYCrvs7X11P') %>% 
    slice(1:100) %>% 
    add_audio_analysis()

Thousand<-
    get_playlist_audio_features(
        'thedoctorkto','54nv8jbrm4JoHEZ49Qvjgl') %>% 
    slice(1:100) %>% 
    add_audio_analysis()

Comparement <-
    Mijn %>% mutate(playlist = "Mijn") %>%
    bind_rows(Thousand %>% mutate(playlist = "Thousand"))
```


```{r}
Comparement %>% 
    mutate(
        sections = 
            map(
                sections, 
                summarise_at, 
                vars(tempo, loudness, duration), 
                list(section_mean = mean, section_sd = sd))) %>% 
    unnest(sections) %>%
    ggplot(
        aes(
            x = tempo, 
            y = tempo_section_sd, 
            colour = playlist, 
            alpha = loudness)) +
    geom_point(aes(size = duration / 60)) + 
    geom_rug() + 
    theme_minimal() +
    ylim(0, 5) + 
    labs(
        x = 'Mean Tempo (bpm)', 
        y = 'SD Tempo', 
        colour = 'Genre', 
        size = 'Duration (min)', 
        alpha = 'Volume (dBFS)')

Comparement%>% 
    mutate(
        timbre =
            map(
                segments,
                compmus_summarise,
                timbre,
                method = 'mean')) %>%
    select(playlist, timbre) %>% 
    compmus_gather_timbre %>% 
    ggplot(aes(x = basis, y = value, fill = playlist)) +
    geom_violin() +
    scale_fill_viridis_d() +
    labs(x = 'Spotify Timbre Coefficients', y = '', fill = 'Playlist')
```
*** 
These low level features show once again that my own playlist is quite different from the pop playlist. 



***
Once again, these low level features show that my playlist is quite different from the popplaylist. 

###Classification (127)

```{r}
SadSongs<- 
    get_playlist_audio_features('spotify', '37i9dQZF1DX7qK8ma5wgG1') %>% 
    slice(1:20) %>% 
    add_audio_analysis
HappyTunes <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DX9u7XXOp0l5L') %>% 
    slice(1:20) %>% 
    add_audio_analysis
Mar19 <- 
    get_playlist_audio_features('11122548577', '6eEo9NEapZSsUPlE7ZSjvU') %>% 
    slice(1:20) %>% 
    add_audio_analysis
```


```{r}
Comparement6 <- 
    SadSongs %>% mutate(playlist = "Sad Songs") %>% 
    bind_rows(
        HappyTunes %>% mutate(playlist = "Happy Tunes"),
        Mar19 %>% mutate(playlist = "Mar19")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)
```


```{r}
Comparement6_class <- 
    recipe(playlist ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = Comparement6) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(Comparement6) %>% 
    juice
```


```{r}
Comparement6_cv <- Comparement6_class %>% vfold_cv(5)
```
```{r}
Comparement6_knn <- nearest_neighbor(neighbors = 1) %>% set_engine('kknn')
predict_knn <- function(split)
    fit(Comparement6_knn, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```



```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class)
```



```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'mosaic')
```

```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'heatmap')
```



```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```


```{r}
Comparement6_logistic <- logistic_reg() %>% set_engine('glm')
predict_logistic <- function(split)
    fit(indie_logistic, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```


```{r}
Comparement6_multinom <- multinom_reg(penalty = 0.1) %>% set_engine('glmnet')
predict_multinom <- function(split)
    fit(Comparement6_multinom, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```


```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_multinom)) %>% unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```


```{r}
Comparement6_class %>% 
    fit(Comparement6_multinom, playlist ~ ., data = .) %>% 
    pluck('fit') %>%
    coef
```


```{r}
Comparement6_tree <- decision_tree() %>% set_engine('C5.0')
predict_tree <- function(split)
    fit(Comparement6_tree, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```

```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_tree)) %>% unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```


```{r}
Comparement6_class %>% 
    fit(Comparement6_tree, playlist ~ ., data = .) %>% 
    pluck('fit') %>%
    summary
```


```{r}
Comparement6_forest <- rand_forest() %>% set_engine('randomForest')
predict_forest <- function(split)
    fit(Comparement6_forest, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
```

```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_forest)) %>% 
    unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```

```{r}
Comparent6_class %>% 
    fit(Comparement6_forest, playlist ~ ., data = .) %>% 
    pluck('fit') %>% 
    randomForest::varImpPlot()
```


```{r}
predict_knn_reduced <- function(split)
    fit(
        Comparement6_knn, 
        playlist ~ c01 + c11 + liveness + energy + acousticness, 
        data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_knn_reduced)) %>% unnest(pred) %>% 
    metric_set(accuracy, kap, j_index)(truth = playlist, estimate = .pred_class)
```

```{r}
Comparement6_cv %>% 
    mutate(pred = map(splits, predict_knn_reduced)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'mosaic')
```


```{r}
Comparement6 %>%
    ggplot(aes(x = c01, y = c11, colour = playlist, size = liveness)) +
    geom_point(alpha = 0.8) +
    scale_color_brewer(type = 'qual', palette = 'Accent') +
    labs(x = 'Timbre Component 1', y = 'Timbre Component 11', size = 'Liveness', colour = 'Playlist')
```



***
**Heartbeat**
The song Please Be Naked is charastic for the use of a heartbeat is prominent beat. The closer to the end of the song, the louder the heartbeat is. 


###Credits


**Student**
Made by Marielle Baelemans, 2019. For info: marielle.baelemans@student.uva.nl. 

**University** 
This portfolio is part of the Computational Musicology course, given by the Musicology department of the University of Amsterdam. 

