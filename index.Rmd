---
title: "Sad Music"
author: "Marielle Baelemans"
date: "31 March 2019"
output: 
  flexdashboard::flex_dashboard
    
   
---

<style>                     
.navbar {
  background-color:black;
  border-color:black;
}
.navbar-brand {
color:white!important;
}

.bgred {
  background-color: #ffffff;
}
</style> 

```{r, cache = FALSE}
 knitr::opts_chunk$set(cache = TRUE)

# In order to use these packages, we need to install flexdashboard, plotly, and Cairo.
library(tidyverse)
library(tidymodels)
library(plotly)
library(dplyr)
library(spotifyr)
library(compmus)
source('spotify.R')
library(protoclust)
library(plotly)
library(heatmaply)
library(ggdendro)
library(protoclust)
library(cluster)

```

Music and Emotions
=============================================


Column {.tabset}
--------------------------------------
### **Introduction**
Due Social Media and her memes we all know some famous songs that are played when a sad scenerio is visualised. But what do these songs have in common that they're associated with a 'sad' feeling? This was my research question in earlier Musicology research. I focused on three aspects of music: pitch, tempo and mode. The results showed that a minor mode, slow tempo and low pitch gives music a 'sad' association, in a 3:2:1 ratio. The violinbargraph is shown under the 'Violin Bar Graph'.

Knowing that, I want to expend my research on sad music. 'You listen to such depressin music!', is something I hear a lot. Once again, what does make my music more sad than others? To compare to others, I'll take popmusic as a comparement, because that is the genre that most listened by the mainstream human being in our society. The main question I try to answer with this project is: **Is my own music taste more sad than the most listened pophits?**

For answering this question I will make use of Spotify. Their playlist are very usefull, some are labeled with a mood. For this research, 'Happy Tunes' and 'Sad Songs' are used to find out what the makers of these list see as happy and as sad. Besides, for the main comparement is the playlist 'BIGGEST HITS OF ALL TIME' used, consisting of 10k popsongs. This one is comparement with the biggest playlist of my own, 'Watvoor muzieksmaak heb je?', consisting of 6k of songs I listen to reguraly. 

### Violin Bar Graph

![ResultsAO](ResultsAO.jpg)

Column {.tabset}
-----------------------

### Meme 

![SadMusicMeme](SadMusicMeme.jpg)



Approaches to sadness
====================

Row 
----------------------------

### Material and method
**1)**First of all, I will see how my own variables **Mode, pitch and tempo** consist in the the Happy and the Sad playlist. Sadly, pitch is not a variable within the Spotify API, so I'll use loudness as an extra variable. Moreover, in this section I also make use of the twelve timbre features of Spotify to compare the 'colorness' of a Sad and a Happy playlist. 

---  *As you can see in this graph, the blue dots (meaning minor) are in the left corner, meaning that the tempo on average and the change within a song is the same: both relatively slow. This is in line with my earlier finding: slow and minor are both aspects of 'Sad' music. In the timbre comparement graph is clear to see that the timbre features differ among the playlists. Mostly, the sad playlist has a lower value on the feature, but as you can see at feature 5, (C05), it is higher than by the Happy playlist.*  


**2)**Secondly, in **cognitive musicology**, mood of music is shaped by *Valence and Energy* . Low valence and low energy is seen as sad music. In this graph I added, once again, the mode in color,the tempo in alpha and the loudness in size.  


---  *As you can see in the graph, this approach is true by the Happy and Sad playlist of Spotify! Moreover, the whole Sad playlist is in under 0.5 in his valence value and mostly under 0,5 in energy. The Happy playlist however, does not have that clear distinction. It is mostly in the top right, meaning high energy and high valence. Some songs of this playlist, who also have a slow tempo, are more to the left corner.* 


**3)**But what does **Spotify** use? The open this black box, I make us of a clustering function. In the dendogram I compare the playlists within themselves to see what Spotify sees as the songs who have the most in common. Secondly I use a heatmaply to see on feature which aspects exactly holds these songs together

--- *Sadly, it seems unable to bring out a clear classification within both playlists. The heatmap works for Happy only. So based on the black box of Spotify will keep closed. *

**All together:**
As you can clearly see out of these graphs, is that Happy Tunes have a high energy and valence level. Sad songs however, have a low energy, low loudness and a lower tempo than the happy tunes. Clear to see that, also in music,  *Sadness comes when everything is gone* . Suprising in both the first and the second tab of graphs is that the minor songs noticable times have higher tempo, valence and energie than the the major mode songs. Seeing this, I personally think of emo/punk songs who are quite dramatic and are reaching for a sad climax. 


---  *Based on this approaches, we can say that sad music (as a playlist) is shaped alow tempo, low energy, low valence and low loudness.*


Column {.tabset}
--------------------------------
### **1) Mode, Pitch and tempo**
```{r}
SadSongs<-
    get_playlist_audio_features(
        'spotify', '37i9dQZF1DX7qK8ma5wgG1') %>% 
          add_audio_analysis()

HappyTunes<-
    get_playlist_audio_features(
        'spotify','37i9dQZF1DX9u7XXOp0l5L') %>% 
          slice(1:60) %>%
         add_audio_analysis()

Comparement <-
  SadSongs %>% mutate(playlist = "Sad") %>%
    bind_rows(HappyTunes %>% mutate(playlist = "Happy"))

Comparement %>% 
    mutate(
        sections = 
            map(
                sections, 
                summarise_at, 
                vars(tempo, loudness, duration), 
                list(section_mean = mean, section_sd = sd))) %>% 
    unnest(sections) %>%
    ggplot(
        aes(
            x = tempo, 
            y = tempo_section_sd, 
            colour = mode, 
            alpha = loudness,
            fill = mode
            )) +
    geom_point(aes(shape= playlist, size= duration)) + 
    geom_rug() + 
    theme_minimal() +
    ylim(0, 5) + 
    labs(
        x = 'Mean Tempo (bpm)', 
        y = 'SD Tempo', 
        colour = 'Mood',
        shape = 'Mood', 
        alpha = 'Loudness',
        size =  'Duration') 



Comparement%>% 
    mutate(
        timbre =
            map(
                segments,
                compmus_summarise,
                timbre,
                method = 'mean')) %>%
    select(playlist, timbre) %>% 
    compmus_gather_timbre %>% 
    ggplot(aes(x = basis, y = value, fill = playlist)) +
    geom_violin() +
    theme_minimal() +
    scale_fill_viridis_d() +
    labs(x = 'Spotify Timbre Coefficients', y = '', fill = 'Playlist')

``` 

### **2) Cognitive Musicology**
````{r}

# Combine data sets with a labelling variable

CognComparement <-
 HappyTunes%>% mutate(playlist = "Happy") %>%
  bind_rows(SadSongs %>% mutate(playlist = "Sad"))

# Code by Ashley -> my research

CognComparement %>%                       # Start with awards.
  ggplot(                      # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = loudness,            #Let op: tempo ipv loudness
      colour = mode,
      alpha= tempo,
    )
  )+
  geom_point() +               # Scatter plot.
  geom_rug(size = 0.1) +       # Add 'fringes' to show data distribution.
  facet_wrap(~ playlist) +     # Separate charts per playlist.
  scale_x_continuous(          # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),  # Use grid-lines for quadrants only.
    minor_breaks = NULL      # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(          # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(         # Use the Color Brewer to choose a palette.
    type = "qual",           # Qualitative set.
    palette = "Set1"       # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(       # Fine-tune the sizes of each point.
    trans = "exp",           # Use an exp transformation to emphasise loud.
    guide = "none"           # Remove the legend for size.
  ) +
  theme_light() +              # Use a simpler them.
  labs(                        # Make the titles nice.
    x = "Valence",
    y = "Energy",
    colour = "Mode",
    size = "Loudness",
    alpha = "Tempo"
  )
```

### **3) Clustering / Dendogram**
```{r}
HappyClust <- 
    get_playlist_audio_features('spotify','37i9dQZF1DX9u7XXOp0l5L') %>% 
    slice(1:10) %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches,  
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)

SadClust <-
      get_playlist_audio_features('spotify','37i9dQZF1DX7qK8ma5wgG1') %>% 
    slice(1:10) %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)

HappyClust_juice <- 
    recipe(track_name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = HappyClust) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(HappyClust%>% mutate(track_name = str_trunc(track_name, 20))) %>% 
    juice %>% 
    column_to_rownames('track_name')

SadClust_juice <- 
    recipe(track_name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = SadClust) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(SadClust%>% mutate(track_name = str_trunc(track_name, 20))) %>% 
    juice %>% 
    column_to_rownames('track_name')
```

###**3) Clustering / Heatmap**
```{r}

grDevices::dev.size("px")

heatmaply(
    HappyClust_juice,
    hclustfun = hclust,
    # hclustfun = protoclust,
    # Comment out the hclust_method line when using protoclust.
    hclust_method = 'average',
    dist_method = 'euclidean')
```

```{r} 
heatmaply(
    SadClust_juice,
    hclustfun = hclust,
    # hclustfun = protoclust,
    # Comment out the hclust_method line when using protoclust.
    hclust_method = 'average', 
    dist_method = 'euclidean')

```


Song to Song comparement
==============================

Column {.tabset}
----------------------------
###Case Study
Knowing what music as a whole makes sad, it is time to find out what music as a song makes sad. In this part I can finally use my 'pitch' variable from earlier research! As a case study I have chosen two songs who were recenlty hits and are associatable with the moods I'm describing. For the happy category I have chosen 'Happy' by Pharrell Williams, cause 'clap along if you feel like happiness is the truth'. For my sad category I make use of 'Say Something' by A Great Big World & Christina Aguilera.

**1)**By using **chromagrams** we are able to see with tones we hear while listening to so the song. 

---  *In the chromogram of 'Happy' we see that the most used tone is F in combination with c#. The tones change fast, meaning that a lot of notes are used to make this song. In 'Say Something' however, the tones do not change as much as in the 'Happy' song. The most used tone is clearly D, followed by A and that is mostly it. *

**2)**In these graphs we can see which chords are used, so we can detect the harmony. 

---*In 'Happy' we see yellow in the major, like D major, B major and E major. This would mean that they are least in the song. This is weird.  We also see phrases in the song, the song seems to have 9 parts. This is in line with earlier finding that happy music has more change in it. In the 'Say Something' graph we see no clear distinction in chords used, only the D major chord is clear darker than the others. There are seven phrares to see, but less clear than by 'Happy'.* 

**3)** To move futher on the speed of a song we look at **tempo and beats**. In this section we have a closer look at how tempo behaves in these two songs. For this we only use 30 seconds of the song, to have a clear close look. As we already have seen in the pitch and chord tabs, the 'Happy' song chances more than the 'Say Something' song. Can we also see this pattern in tempo related ways? For answering this I make use of three kinds of measuring: 

a. Novelty function (to see how new a beat is), 
b. beat detenction based on the tones and 
c. the change in the 12 timbre functions of Spotify.

--- *Looking at 'Happy' by Pharrell we see in the novelty function that there are a lot of peaks, sharp and around 15. In the pitch based tempogram we see the same pattern as in the chromagram, fast changes over all the twelves tones. In the timbre based tempogram we see, suprising, the same. The first feature is stable, so the loudness does not change. the second and the third however, change over time, in a fast way. In the novelty function of 'Say Something' we see peaks with much width, and way less than by 'Happy'. In the end of this 30 seconds fragment, we see a big one, meaning that there is a new beat incoming. In the tempogram based on pitch we slower pattern, as we expected from the chromagrams. Same for the timbrefeatures, it seems that this sad song does not change in his timbre.* 


Column {.tabset}
--------------------------------

### **1) Pitch in Chromagrams**
 
 
 
```{r}
  HappyPharell <- 
    get_tidy_audio_analysis('6NPVjNh8Jhru9xOmyQigds') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)

  SaySomething <- 
    get_tidy_audio_analysis('6Vc5wAMmXdKIAM7WUoEb7N') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)

HappyPharell %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
      aes(
        x = start + duration / 2, 
        width = duration, 
        y = pitch_class, 
        fill = value
        )) + 
    geom_tile() +
    labs(x = 'Time (s)', y = 'Happy', fill = 'Magnitude') +
    theme_minimal()


SaySomething %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
      aes(
        x = start + duration / 2, 
        width = duration, 
        y = pitch_class, 
        fill = value
        )) + 
    geom_tile() +
    labs(x = 'Time (s)', y = 'Say Something', fill = 'Magnitude') +
    theme_minimal()
```
```{r}

circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))


```


### **2) Harmony in chordograms**


```{r}
HappyPharell2<- 
    get_tidy_audio_analysis('6NPVjNh8Jhru9xOmyQigds') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))

SaySomething2<- 
    get_tidy_audio_analysis('6Vc5wAMmXdKIAM7WUoEb7N') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))


HappyPharell2 %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'Happy')

SaySomething2 %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'Say Something')

```


```{r}
HappyPharell3 <- 
    get_tidy_audio_analysis('6NPVjNh8Jhru9xOmyQigds') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))

```


### **3) Tempo in tempogram**



```{r}
HappyPharell3plot <- 
    bind_rows(
        HappyPharell3 %>% compmus_self_similarity(pitches, 'aitchison') %>% mutate(d = d / max(d), type = "Chroma"),
        HappyPharell3 %>% compmus_self_similarity(timbre, 'euclidean') %>% mutate(d = d / max(d), type = "Timbre")) %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    facet_wrap(~ type) +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = 'Happy')

SaySomething3 <- 
    get_tidy_audio_analysis('6Vc5wAMmXdKIAM7WUoEb7N') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))

SaySomething3plot <- 
    bind_rows(
        SaySomething3 %>% compmus_self_similarity(pitches, 'aitchison') %>% mutate(d = d / max(d), type = "Chroma"),
        SaySomething3 %>% compmus_self_similarity(timbre, 'euclidean') %>% mutate(d = d / max(d), type = "Timbre")) %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    facet_wrap(~ type) +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = 'Say Something')

```


```{r}
HappyPharell4 <- 
    get_tidy_audio_analysis('6NPVjNh8Jhru9xOmyQigds') %>% 
    select(segments) %>% unnest(segments)

HappyPharell4 %>% 
    mutate(loudness_max_time = start + loudness_max_time) %>% 
    arrange(loudness_max_time) %>% 
    mutate(delta_loudness = loudness_max - lag(loudness_max)) %>% 
    ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
    geom_line() +
    xlim(0, 30) +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'Novelty')

HappyPharell4 %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    arrange(start) %>% 
    mutate(pitches = map2(pitches, lag(pitches), `-`)) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = pmax(0, value))) + 
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    xlim(0, 30) +
    labs(x = 'Time (s)', y = 'Happy', fill = 'Magnitude') +
    theme_classic()

HappyPharell4%>% 
    arrange(start) %>% 
    mutate(timbre = map2(timbre, lag(timbre), `-`)) %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = pmax(0, value))) + 
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    xlim(0, 30) +
    labs(x = 'Time (s)', y = 'Happy', fill = 'Magnitude') +
    theme_classic()

SaySomething4 <- 
    get_tidy_audio_analysis('6Vc5wAMmXdKIAM7WUoEb7N') %>% 
    select(segments) %>% unnest(segments)

SaySomething4 %>% 
    mutate(loudness_max_time = start + loudness_max_time) %>% 
    arrange(loudness_max_time) %>% 
    mutate(delta_loudness = loudness_max - lag(loudness_max)) %>% 
    ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
    geom_line() +
    xlim(0, 30) +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'Novelty')

SaySomething4 %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    arrange(start) %>% 
    mutate(pitches = map2(pitches, lag(pitches), `-`)) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = pmax(0, value))) + 
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    xlim(0, 30) +
    labs(x = 'Time (s)', y = 'Say Something', fill = 'Magnitude') +
    theme_classic()

SaySomething4%>% 
    arrange(start) %>% 
    mutate(timbre = map2(timbre, lag(timbre), `-`)) %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = pmax(0, value))) + 
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    xlim(0, 30) +
    labs(x = 'Time (s)', y = 'Say Something', fill = 'Magnitude') +
    theme_classic()
```



My Own vs Pop 
======================================

Column {.tabset}
-------------------------
###**Results**
After having all these ways to approach 'Sadness' in music, it is time to compare my own playlist to popmusic. Because we are talking about music as a whole playlist, we have to fit with our measure variables. Out of the approaches I prefer the cognitive most, because that one gives the clearest distinction between the 'Happy Tunes' and 'Sad Songs' playlist. In this graph  I also included mode in the color, tempo in alpha and loudness in size. 

--- *By using so many songs, we get an abstract painting as graph! While the popplaylist makes a line from the left corner to the right corner above with being a bit more to the left, my own playlist is way more to the left. My own music has a group in completely in the left corner, the pop one does not have this. There is also a cluster in the left corner above, low valence and high energy. This is associated with anger.  It seems that my own playlist has relatively more blue spots than red, compared to the pop one. This means that a bigger percentage is minor than in pop music. This is honestly hard to tell, due the amount of data.*

Column {.tabset}
----------------------------

###**Putting all the variables together**
````{r}


# Download the playlists for the comparement

Pop <- get_playlist_audio_features('thedoctorkto','54nv8jbrm4JoHEZ49Qvjgl') %>% slice(1:5000)
Own<- get_playlist_audio_features('11122548577','76MRLl4f6AvyYCrvs7X11P') %>% slice(1:5000)

# Combine data sets with a labelling variable

Mood <-
  Pop %>% slice(1:5000) %>% mutate(playlist = "Pop") %>%
  bind_rows( Own %>% slice(1:5000) %>% mutate(playlist = "Own"))

# Code by Ashley -> my research

Mood %>%                       # Start with awards.
  ggplot(                      # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = loudness,            #Let op: tempo ipv loudness
      colour = mode,
      alpha= tempo,
    )
  )+
  geom_point() +               # Scatter plot.
  geom_rug(size = 0.1) +       # Add 'fringes' to show data distribution.
  facet_wrap(~ playlist) +     # Separate charts per playlist.
  scale_x_continuous(          # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),  # Use grid-lines for quadrants only.
    minor_breaks = NULL      # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(          # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(         # Use the Color Brewer to choose a palette.
    type = "qual",           # Qualitative set.
    palette = "Set1"       # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(       # Fine-tune the sizes of each point.
    trans = "exp",           # Use an exp transformation to emphasise loud.
    guide = "none"           # Remove the legend for size.
  ) +
  theme_light() +              # Use a simpler them.
  labs(                        # Make the titles nice.
    x = "Valence",
    y = "Energy",
    colour = "Mode"
  )  

```




And so...
======================

Column {.tabset}
-----------------------------

###Conclusion
>* Putting this all together, we can conclude that my own playlist --and so my own music taste-- is more sad compared to the big popplaylist.

  + Moverover I learned some extra things about sad music. 
      + It seems that Happy music has more change in tonal structure than sad music does.
      + Sad music uses less different tones than happy music
      + Timbre has also influence on mood and music. For now it seems that sad music has less timbre than happy music has. Timbre is quite a black box, but interesting for futher research.
      
### Discussion
>It is always nice to have scientific evidence behind common sayings. Now that people say that I 'listen to such depressic music?!' I can say they are right, and annoy them with statistical evidence. I know more about what makes music sad, but I am far away from being done. 

Timbre is a black box what needs to be revealed. Also, I did only a single song comparement between two songs, so that one can not  be generalized yet. It gives us a good start however, including more knowledge about these songs particular. The comparement graph between my own music and the popmusic is not that clear as I wanted to be. 
    
      
  

Column {.tabset}
----------------------
### Meme
![Another meme](SadMusicMeme2.jpg)

Appendix
=======================

>This portfolio is made by Mariëlle Baelemans, student (11853565) at the University of Amsterdam for the course Computational Musicology. For more info: marielle.baelemans@student.uva.nl

Happy Tunes by Spotify: https://open.spotify.com/user/spotify/playlist/37i9dQZF1DX9u7XXOp0l5L?si=G6uOsSrIQdKpoHMDQO2Hgg

Sad Songs by Spotify: https://open.spotify.com/user/spotify/playlist/37i9dQZF1DX7qK8ma5wgG1?si=NQJoYoGGToOJz1pV7XZCLw

'Watvoor muzieksmaak heb je?'by Mariëlle Baelemans: https://open.spotify.com/user/11122548577/playlist/76MRLl4f6AvyYCrvs7X11P?si=mpGysdvXQruWI5-58WtN3w

BIGGEST PLAYLIST WITH ALL THE BEST SONGS by Maxim Averbukh :https://open.spotify.com/user/thedoctorkto/playlist/54nv8jbrm4JoHEZ49Qvjgl?si=G3nOcOMcTjqex2M6lrYmcw

